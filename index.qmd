---
title: "Untitled"
format: html
editor: visual
---

---
title: "<p style=\"color:black,text-align:center\">Basic Data Analysis</p>"
author: 
  - name: <font color=#ff6600><b>Biometrics Unit</b></font> 
    affiliation: <font color=#ff6600><b>International Institute of Tropical Agriculture (IITA)</b></font>
format:
  html:
    html-math-method: mathjax
    css: "style.css"
    toc: true
    toc-location: right
    toc_float:
    collapsed: True
    smooth_scroll: true
    code_folding: show
    number_sections: false
    theme: readable
    highlight: monochrome
    fig_width: 5
    fig_height: 5
    fig_caption: true
    df_print: paged
    xaringan::moon_reader: null
    lib_dir: libs
    nature:
      highlightStyle: github
---

# [**Summarizing Quantitative Data**]{style="color: #2C6D26;"}

## [Central Tendency]{style="color: #002D62;"}

Central tendency is an important statistical measure of quantitative data. It identifies a single value as representative of an entire distribution of the collected data. The **Mean**, **Median**, and **Mode** are three of the common ways to measure central tendency. Other measurements, such as a trimmed Mean, weighted Mean, geometric Mean, and harmonic Mean, are not covered in this course.

### **Mean**

The most used measure of central tendency is the Mean. The average of the data is the Mean. It is computed by adding all the values in the data set divided by the number of observations in it. Extreme values can easily alter the Mean, which is one of the Mean's shortcomings. i.e. sensitive to extreme values.

To calculate the Mean, you first add all the numbers together (3 + 11 + 4 + 6 + 8 + 9 + 6 = 47). Then you divide the total sum by the number of scores used (47 / 7 = 6.7). In this example, the Mean or average of the number set is 6.7.

Use the Mean to describe the sample with a single value that represents the center of the data. Many statistical analyses use the Mean as a standard measure of the center of the distribution of the data. The Median and the Mean both measure central tendency.

### **Median**

The Median is the value in the ordered data that is in the middle. The most critical step in calculating the Median is to arrange the data in ascending order from least to largest. Extreme values can not easily alter the Median. i.e. resistant to extreme values. When the dataset contains an even number of values, then the Median value of the dataset can be found by taking the Mean of the middle two values

### **Mode**

The Mode is the most often occurring value in the data. It's crucial to keep in mind that the dataset could have multiple Modes. Extreme values can not easily alter the Mode. i.e. resistant to extreme values.

Let's import the needed data with this command to calculate the Mean, and Median.

```{r}
library(readxl) 
dat <- read_excel("Data Summary.xlsx", sheet="Mean")   
mean(dat$X)   
median(dat$X) 
```

Let's see what happens to the Mean and Median if we mistakenly replace a number with an extreme value

```{r}
mean(dat$Y)  
```

The extreme value distorts the Mean.

```{r}
median(dat$Y) 
```

The Median is not really affected by the extreme value because of its characteristics of `resistant to extreme values`.

Measures of central tendency are chosen based on the characteristics of the data.

-   All three central tendency measurements hold true for continuous data with a symmetrical distribution. However, because the Mean takes into account every value in the dataset or distribution, the analyst typically employs it.

-   The Median is the most accurate way to determine the central tendency in a skewed distribution.

-   The best options for determining the central tendency, if you have the original data, are the Median and the Mode.

-   The Mode is the most effective way to determine the central tendency when dealing with categorical data.

## [Measure of Spread (Variability/Dispersion)]{style="color: #002D62;"}

A summary statistic that indicates the amount of dispersion in a dataset is known as a measure of spread. Measures of spread describe how similar or varied the set of observed values is for a specific variable (data item). Summarising the dataset can help us understand the data, especially when the dataset is large.

**Variance**, **standard deviation**, and **standard error** are three of the commonly used measures of spread.

Spread is in the context of the distribution of dataset points. A low value of spread indicates that the data points tend to be clustered tightly around the center while a high value of spread indicates the data points are far away from the center. To demonstrate the idea of spread, let's consider these two different variables

```{r}
library(readxl) 
dat1 <- read_excel("Data Summary.xlsx", sheet="Spread") 
mean(dat1$spread01)  
median(dat1$spread01) 
```

```{r}
library(readxl) 
dat1 <- read_excel("Data Summary.xlsx", sheet="Spread")   
mean(dat1$spread02)   
median(dat1$spread02)
```

The two datasets have the same center, but how spread out are the values?. While a measure of central tendency describes the typical values, measures of spread define how far away the data points are from the center.

Let’s examine the spread of the above variables

```{r}
var(dat1$spread01)   
var(dat1$spread02) 
```

We can see the variability in how spread out the data points are from each other.

```{r}
sd(dat1$spread01) 
sd(dat1$spread02) 
```

This also shows the difference in their standard deviations.

# [**Summarizing Qualitative Data**]{style="color: #2C6D26;"}

Once we have determined a variable to be qualitative (categorical), we need tools to summarize the data.

-   The best way to summarize categorical data is to use frequencies (counts) and percentages (proportions).

-   We can also summarize qualitative data by graphing the data using bar plots, histogram etc.

## **Frequencies (counts)**

The quantity or the total number of an item in a collection or a group is simply referred to as frequency. The number of times an event or observation occurred during an experiment or research is known as its frequency in statistics. It can alternatively be understood as the simple count of an occurrence.

## **Percentages (proportions)**

A percentage is a fraction or part of the total that possesses a certain characteristic.

Let’s consider an example:

In a survey of 20 participants, a question was asked about their choice of a political party. Their responses are in a vector called *pol_party*

```{r}
library(readxl) 
dat2 <- read_excel("Data Summary.xlsx", sheet="Poll") 
```

We can easily summarize their responses within a few minutes, but it becomes difficult when the number of respondents is 100 and above.

We can use the function **table()** and **prop.table()** to obtain the frequency and proportion.

```{r}
#Frequency  
table(dat2$pol_party) 
```

```{r}
#Proportion  
prop.table(table(dat2$pol_party)) 
```

# [**Data Summary: Practical**]{style="color: #2C6D26;"}

-   Import **Example-02.csv** into R and save in an object named **example02**

-   filter the object using the year `1970` and save it to a new object named **example02.70**

-   using the new object, summarize:

-   the qualitative variables

-   the first three quantitative variables

# [**Correlation**]{style="color: #2C6D26;"}

Correlation is a bivariate analysis that measures the strength of association between two variables. In statistics, the value of the correlation coefficient varies between -1 to +1. When the value of the correlation coefficient lies around ± 1, then it is said to be a perfect degree of association between the two variables. As the correlation coefficient value goes towards 0, the relationship between the two variables is weak.

A common statistic to show the strength of a relationship that exists between two continuous variables is called the Pearson correlation coefficient or just correlation coefficient. We may want to calculate the correlation coefficient between height and weight for example.

Two common types of correlation tests will be discussed in this session:

-   Pearson’s correlation

-   Spearman’s correlation

The function **cor.test()** in base R can be used to test for association/correlation.

The arguments needed are:

-   x, y: Numeric vectors of the same length
-   method: Correlation method i.e. *("pearson", "spearman")*

## [Pearson’s Correlation]{style="color: #002D62;"}

Pearson’s correlation is a parametric test widely used in statistics to measure the degree of the relationship between linear related variables and both variables should be normally distributed.

For illustration, let's consider the `australia.soybean` data, a multi-environment trial of 58 varieties of soybeans, in 4 locations across 2 years in Australia and examine the relationship/association between `yield` and `oil`.

```{r }
#|message=FALSE
#Pearson correlation 
library(tidyverse) 
dat3 <- read_csv("summary.australia.soybean.csv")
cor.test(dat3$yield, dat3$oil, method= "pearson")
```

This examines the relationship between yield and oil of the Australian soybean. The test statistic, t(8.82) indicates how many standard deviations away from zero the sample is, with p-vale (3.528e-12) indicating that there is a statistically significant correlation between the yield and oil of the Australian soybean.The sample estimates correlation coefficient (0.7625) which is very close to 1, indicates a strong positive linear relationship between the yield and oil of the soybean.

## [Spearman’s Correlation]{style="color: #002D62;"}

Spearman’s rank-order correlation coefficient is a rank-based version of Pearson’s correlation coefficient. It does not rely on any assumption about the distributions of the variables. It is a measure of rank correlation, i.e., the similarity of the orderings of the data when ranked by each of the quantities.

Let's examine the association between `Height` and `DaystoFlower`, assuming the data are not normally distributed.

```{r}
#Spearman correlation  
library(readxl) 
dat4 <- read_excel("correlations.xlsx") 

cor.test(dat4$Height, dat4$DaystoFlower, method="spearman")  
```

This examines the relationship between height and days to flowering of the plant. The test statistic S(1169.8), in the context of Spearman's correlation, this value relates to the sum of the ranks associated with the differences between the ranks of the two variables. The p-value is reported as 0.2938 implying that there is no statistical significance between height and days to flowering of the plant. The sample estimates of the Spearman correlation coefficient $p$ (0.2404). This suggests a weak positive correlation between the variables, indicating that as one increases, the other tends to also increase, but the relationship is not strong.

## [Multiple Correlation]{style="color: #002D62;"}

So far, we have seen how to correlate two quantitative variables together using the Pearson and Spearman rank method. It is also possible to correlate multiple quantitative variables together using either of the methods described above depending on the underlying assumption. We recommend the function **rcorr()** in the library **Hmisc** to examine the multiple association/correlation for all possible pairs of quantitative variables.

The arguments needed are:

-   x, y: Numeric matrix with at least five rows and atleast two columns

-   type: Specify the correlation method i.e. *("pearson", "spearman")*

The `rcorr` function returns three outputs, `r` the correlation matrix, `n` the matrix of number of observation used in each pair of variables, and `p` the p-values.

To illustrate this concept, let's consider the `australia.soybean` data in object named `dat3`, and examine the relationship/association for all the possible pairs of the following variables `yield`, `height`, `lodging`, `size`, `protein`, and `oil`.

```{r}
#| message=FALSE
#Pearson correlation  
library(Hmisc)  
dat3 |>  #the dataset  
  as_tibble() |>   
  select(-1) |> #deselect the non-numeric variable in the first column   
  as.matrix() |> #convert the dataframe to matrix
  rcorr(type = "pearson") #perform the correlation using the pearson correlation method dat3 
```

Let us assume the quantitative variables are not normally distributed.

```{r}
#| message=FALSE
#Spearman correlation  
library(Hmisc)  
dat3 |> #the dataset   
  select(-1) |> #deselect the non-numeric variable in the first column   
  as.matrix() |> #convert the dataframe to matrix
  rcorr(type = "spearman") #perform the correlation using the spearman rank correlation method
```

This shows the correlation matrix with yield, height, lodging, size, protein and oil as the variables. The numbers in the matrix reveal the following correlations:

-   yield and height: -0.44 (Moderate negative correlation)
-   yield and lodging: -0.65 (strong negative correlation)
-   yield and size: 0.71 (strong positive correlation)
-   yield and protein: -0.64 (strong negative correlation)
-   yield and oil: 0.74 (strong positive correlation)
-   height and lodging: 0.84 (strong positive correlation)
-   size and oil: 0.94 (very strong positive correlation)

Since all p-values reported are very low, this suggests that all correlations calculated in the matrix are statistically significant.
